@echo off
echo =============================================
echo  ADVANCED HYPERPARAMETER TUNING - START
echo =============================================
echo.
echo ğŸ¯ Starting comprehensive hyperparameter optimization
echo.
echo Features to optimize:
echo   ğŸ“Š Learning rates, dropout, weight decay
echo   âš¡ Gradient clipping and scheduler parameters
echo   ğŸ¨ Progressive training strategies
echo   ğŸ“± Advanced features (MixUp, EMA, etc.)
echo   ğŸ”§ Loss function weights and parameters
echo.
echo â±ï¸  Each trial: 10 epochs (vs 60 in full training)
echo ğŸ¯ Target: 100 trials total
echo â° Expected time: 2-4 hours (depending on GPU)
echo ğŸ“Š Results continuously saved to tune_runs/
echo â¹ï¸  Use Ctrl+C to stop early if satisfied
echo.
echo ğŸ“š Using virtual environment Python...
echo.

venv\Scripts\python.exe training/tune_advanced.py ^
    --n_trials 100 ^
    --study_name attention_unet_advanced ^
    --n_jobs 1

echo.
echo âœ… TUNING COMPLETE!
echo.
echo ğŸ“Š Check results in tune_runs/:
echo   - attention_unet_advanced_best_params.json (optimal parameters)
echo   - attention_unet_advanced_all_results.json (all trial results)
echo   - attention_unet_advanced.db (study database)
echo.
echo ğŸ¯ Use the best parameters in your training script!
echo ğŸ’¡ To continue optimization, use tune_resume.bat
echo.
pause 