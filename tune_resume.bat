@echo off
echo =============================================
echo  ADVANCED HYPERPARAMETER TUNING - RESUME
echo =============================================
echo.
echo ğŸ”„ Resuming hyperparameter optimization from existing study
echo.
echo ğŸ“‹ Resume Features:
echo   âœ… Continues from existing successful trials
echo   ğŸ”„ Automatically reruns failed/pruned trials  
echo   ğŸ“Š Only counts successful trials toward 100 target
echo   ğŸ’¾ Preserves all trial history and best parameters
echo.
echo â¹ï¸  Use Ctrl+C to stop when satisfied with results
echo.
echo â±ï¸  Each trial: 24 epochs (vs 120 in full training)
echo ğŸ“Š Progressive Resizing: start_size â†’ end_size (full size at epoch 19)
echo ğŸ›‘ Early Stopping: 8 epochs patience (33% of trial duration)
echo ğŸ“Š Results continuously saved to tune_runs/
echo ğŸ¯ Target: 100 SUCCESSFUL trials total
echo.
echo ğŸ§¹ Note: Failed trial checkpoints will be cleaned for rerun
echo.
echo ğŸ“š Using virtual environment Python...
echo.

venv\Scripts\python.exe training/tune_advanced.py ^
    --n_trials 100 ^
    --study_name attention_unet_advanced ^
    --n_jobs 1 ^
    --resume

echo.
echo âœ… RESUMED TUNING COMPLETE!
echo.
echo ğŸ“Š Study progress updated - check tune_runs/ for:
echo   - Latest best parameters (JSON)
echo   - All trial results with full history
echo   - Study database with complete optimization path
echo.
echo ğŸ¯ Use the latest best parameters for optimal performance!
echo ğŸ’¡ If more trials needed, run this script again!
echo.
echo ğŸ“ Note: Failed trials have been rerun automatically
echo     Only successful trials count toward the 100 target
echo.
pause 