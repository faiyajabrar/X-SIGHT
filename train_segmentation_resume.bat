@echo off
echo =============================================
echo  RESUME OPTIMIZED ATTENTION U-NET TRAINING
echo  ğŸ† TUNED HYPERPARAMETERS (Trial #281)
echo =============================================
echo.
echo ğŸ”„ Resuming training from existing checkpoint
echo ğŸ† OPTIMIZED HYPERPARAMETERS:
echo   ğŸ“š Learning Rate: 2.11e-4 (vs 3e-4 default)
echo   ğŸ¯ Dropout: 0.09 (vs 0.1 default) 
echo   âš–ï¸  Weight Decay: 1.04e-3 (vs 1e-4 default)
echo   âœ‚ï¸  Gradient Clipping: 1.83 (vs 1.0 default)
echo.
echo ğŸ“‹ Resume Features:
echo   âœ… Continues from last saved checkpoint
echo   ğŸ”„ Preserves training state and metrics
echo   ğŸ“Š Maintains data splits and augmentation state
echo   ğŸ’¾ Resumes learning rate scheduling
echo.
echo âœ… OPTIMIZED CONFIGURATION:
echo   ğŸ§  Attention U-Net (ResNet34 + attention gates)
echo   ğŸ“Š Hybrid Loss (Dice + Focal + Boundary)
echo   âš¡ AdamW + OneCycleLR (superconvergence)
echo   ğŸ“ˆ Progressive resizing (128â†’256px, full size at epoch 84)
echo   ğŸ¨ Progressive augmentations
echo   ğŸ”„ MixUp data augmentation (30%% batches)
echo   ğŸ“± Exponential Moving Average (EMA)
echo   ğŸ“‹ Stochastic Weight Averaging (SWA)
echo   ğŸ¯ Class-wise performance monitoring
echo   â° Early stopping + gradient clipping
echo.
echo â±ï¸  Training Duration: 120 epochs (optimal for OneCycleLR)
echo ğŸ“Š Progressive Resizing: 128px â†’ 256px (full size at epoch 84)
echo ğŸ›‘ Early Stopping: 30 epochs patience (25% of total)
echo ğŸ”„ SWA Start: Epoch 60 (50% of training)
echo.
echo ğŸ“š Using virtual environment Python...
echo.

venv\Scripts\python.exe training/train.py ^
    --lr 2.11e-4 ^
    --dropout 0.09 ^
    --batch_size 16 ^
    --epochs 120 ^
    --weight_decay 1.04e-3 ^
    --warmup_epochs 5 ^
    --early_stopping_patience 30 ^
    --grad_clip_val 1.83 ^
    --use_swa ^
    --swa_start_epoch 60 ^
    --resume

echo.
echo âœ… RESUMED TRAINING COMPLETE!
echo.
echo ğŸ“Š Results Summary:
echo   - Training continued from previous checkpoint
echo   - Best checkpoint updated with highest validation Dice
echo   - Separate checkpoint for best rare class (Dead cells) performance  
echo   - SWA model available for final deployment
echo   - Class-wise performance logged for each epoch
echo.
echo ğŸš€ Next Steps:
echo   1. Check lightning_logs/ for detailed metrics
echo   2. Best model: advanced-*.ckpt files
echo   3. SWA model typically performs best for final evaluation
echo.
pause 